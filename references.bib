@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{eckert2005variation,
  title={Variation, convention, and social meaning},
  author={Eckert, Penelope},
  booktitle={Annual meeting of the Linguistic Society of America. Oakland CA},
  volume={7},
  year={2005}
}

@article{wigboldus_how_2000,
    title = {How do we communicate stereotypes? Linguistic bases and inferential consequences.},
    volume = {78 1},
    pages = {5--18},
    journaltitle = {Journal of personality and social psychology},
    author = {Wigboldus, D. and Semin, G. and Spears, R.},
    date = {2000},
}


@inproceedings{bauer-etal-2018-commonsense,
    title = "Commonsense for Generative Multi-Hop Question Answering Tasks",
    author = "Bauer, Lisa  and
      Wang, Yicheng  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1454",
    doi = "10.18653/v1/D18-1454",
    pages = "4220--4230",
    abstract = "Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model{'}s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.",
}

@inproceedings{andreas-2022-language,
    title = "Language Models as Agent Models",
    author = "Andreas, Jacob",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.423",
    doi = "10.18653/v1/2022.findings-emnlp.423",
    pages = "5769--5779",
    abstract = "Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them{---}a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents{'} communicative intentions influence their language. I survey findings from the recent literature showing that{---}even in today{'}s non-robust and error-prone models{---}LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.",
}


@article{gao_predicting_2019,
    title = {Predicting and Analyzing Language Specificity in Social Media Posts},
    volume = {33},
    rights = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
    issn = {2374-3468},
    url = {https://aaai.org/ojs/index.php/AAAI/article/view/4605},
    doi = {10.1609/aaai.v33i01.33016415},
    abstract = {In computational linguistics, specificity quantifies how much detail is engaged in text. It is an important characteristic of speaker intention and language style, and is useful in {NLP} applications such as summarization and argumentation mining. Yet to date, expert-annotated data for sentence-level specificity are scarce and confined to the news genre. In addition, systems that predict sentence specificity are classifiers trained to produce binary labels (general or specific).We collect a dataset of over 7,000 tweets annotated with specificity on a fine-grained scale. Using this dataset, we train a supervised regression model that accurately estimates specificity in social media posts, reaching a mean absolute error of 0.3578 (for ratings on a scale of 1-5) and 0.73 Pearson correlation, significantly improving over baselines and previous sentence specificity prediction systems. We also present the first large-scale study revealing the social, temporal and mental health factors underlying language specificity on social media.},
    pages = {6415--6422},
    number = {1},
    journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
    author = {Gao, Yifan and Zhong, Yang and Preoţiuc-Pietro, Daniel and Li, Junyi Jessy},
    date = {2019-07-17},
    langid = {english},
    note = {Number: 01},
}


@incollection{maass_linguistic_1999,
    title = {Linguistic Intergroup Bias: Stereotype Perpetuation Through Language},
    volume = {31},
    url = {http://www.sciencedirect.com/science/article/pii/S0065260108602725},
    shorttitle = {Linguistic Intergroup Bias},
    abstract = {Language is considered as the major means by which stereotypes are communicated through interpersonal discourse, by which they are transmitted from generation to generation, and by which the press and other mass media create social representations of social groups. Language also constitutes the principal means by which sociologists and social-psychologists tend to measure stereotypes. Language abstraction plays a subtle but important role in stereotype transmission and maintenance. The aim of this chapter is to describe language abstraction and its development over the past years. The chapter discusses its implications and proposes extensions of the model to related areas. The linguistic intergroup bias ({LIB}) model describes a systematic bias in language use, which can contribute to the perpetuation of stereotypes. The chapter describes the research paradigm most frequently employed in {LIB} studies, reviews empirical findings testing the main predictions, addresses questions of external validity, compares hypotheses about the underlying mechanisms of the {LIB}, speculate about extensions of the model beyond intergroup relations, and finally discusses a number of open problems that are of interest.},
    pages = {79--121},
    booktitle = {Advances in Experimental Social Psychology},
    publisher = {Academic Press},
    author = {Maass, Anne},
    editor = {Zanna, Mark P.},
    date = {1999-01-01},
    langid = {english},
    doi = {10.1016/S0065-2601(08)60272-5},
}


@inproceedings{zellers-etal-2021-turingadvice,
    title = "{T}uring{A}dvice: A Generative and Dynamic Evaluation of Language Use",
    author = "Zellers, Rowan  and
      Holtzman, Ari  and
      Clark, Elizabeth  and
      Qin, Lianhui  and
      Farhadi, Ali  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.386",
    doi = "10.18653/v1/2021.naacl-main.386",
    pages = "4856--4880",
    abstract = "We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today{'}s models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, T5, writes advice that is at least as helpful as human-written advice in only 14{\%} of cases; a much larger non-finetunable GPT3 model does even worse at 4{\%}. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.",
}

@book{austin1975things,
  title={How to do things with words},
  author={Austin, John Langshaw},
  volume={88},
  year={1975},
  publisher={Oxford university press}
}

@inproceedings{govindarajan-etal-2023-counterfactual,
    title = "\href{https://aclanthology.org/2023.findings-acl.813}{Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias}",
    author = "Govindarajan, Venkata S  and
      Beaver, David  and
      Mahowald, Kyle  and
      Li, Junyi Jessy",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.813",
    pages = "12853--12862",
    abstract = "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts {---} thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR reliably use affect in classification, the model{'}s usage of specificity is inconclusive.",
}

@inproceedings{govindarajan-etal-2023-people,
    title = "\href{https://aclanthology.org/2023.eacl-main.183}{How people talk about each other: Modeling Generalized Intergroup Bias and Emotion}",
    author = "Govindarajan, Venkata S  and
      Atwell, Katherine  and
      Sinno, Barea  and
      Alikhani, Malihe  and
      Beaver, David  and
      Li, Junyi Jessy",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.183",
    pages = "2488--2498",
    abstract = "Current studies of bias in NLP rely mainly on identifying (unwanted or negative) bias towards a specific demographic group. While this has led to progress recognizing and mitigating negative bias, and having a clear notion of the targeted group is necessary, it is not always practical. In this work we extrapolate to a broader notion of bias, rooted in social science and psychology literature. We move towards predicting interpersonal group relationship (IGR) - modeling the relationship between the speaker and the target in an utterance - using fine-grained interpersonal emotions as an anchor. We build and release a dataset of English tweets by US Congress members annotated for interpersonal emotion - the first of its kind, and {`}found supervision{'} for IGR labels; our analyses show that subtle emotional signals are indicative of different biases. While humans can perform better than chance at identifying IGR given an utterance, we show that neural models perform much better; furthermore, a shared encoding between IGR and interpersonal perceived emotion enabled performance gains in both tasks.",
}

@inproceedings{venkat2020advice,
    title = {\href{https://www.aclweb.org/anthology/2020.emnlp-main.427/}{Help! Need Advice on Identifying Advice}},
    author = "Govindarajan, Venkata S  and
      Chen, Benjamin  and
      Warholic, Rebecca  and
      Erk, Katrin  and
      Li, Junyi Jessy",
    booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    month = nov,
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.emnlp-main.427},
    doi = {10.18653/v1/2020.emnlp-main.427},
    pages = {5295--5306},
    abstract = {Humans use language to accomplish a wide variety of tasks - asking for and giving advice being one of them. In online advice forums, advice is mixed in with non-advice, like emotional support, and is sometimes stated explicitly, sometimes implicitly. Understanding the language of advice would equip systems with a better grasp of language pragmatics; practically, the ability to identify advice would drastically increase the efficiency of advice-seeking online, as well as advice-giving in natural language generation systems. We present a dataset in English from two Reddit advice forums - r/AskParents and r/needadvice - annotated for whether sentences in posts contain advice or not. Our analysis reveals rich linguistic phenomena in advice discourse. We present preliminary models showing that while pre-trained language models are able to capture advice better than rule-based systems, advice identification is challenging, and we identify directions for future research.},
}

@article{genericity,
    author = {Govindarajan, Venkata and Durme, Benjamin Van and White, Aaron Steven},
    title = {\href{https://doi.org/10.1162/tacl_a_00285}{Decomposing Generalization: Models of Generic , Habitual, and Episodic Statements}},
    journal = {Transactions of the Association for Computational Linguistics (TACL)},
    volume = {7},
    number = {},
    pages = {501-517},
    year = {2019},
    abstract = { We present a novel semantic framework for modeling linguistic expressions of generalization— generic, habitual, and episodic statements—as combinations of simple, real-valued referential properties of predicates and their arguments. We use this framework to construct a dataset covering the entirety of the Universal Dependencies English Web Treebank. We use this dataset to probe the efficacy of type-level and token-level information—including hand-engineered features and static (GloVe) and contextual (ELMo) word embeddings—for predicting expressions of generalization. },
}
